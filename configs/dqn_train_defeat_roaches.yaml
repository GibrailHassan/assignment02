# Trains a DQN agent on the DefeatRoachesEnv.
# This configuration replaces the 'run_dr_env.py' script.

experiment_name: "DQN on DefeatRoaches"

# --- Environment Configuration ---
# Specifies the environment to use and its parameters.
environment:
  name: "DefeatRoachesEnv" # The environment for the DefeatRoaches mini-game.
  params:
    screen_size: 32 # Screen resolution (e.g., 32x32).
    step_mul: 8 # Number of game steps per agent action.
    is_visualize: false # Set to true to watch training (will be very slow).
    # reselect_army_each_nstep: 5 # This was a parameter in the original dr_env.py.
    # It can be added here if your DefeatRoachesEnv class still supports it.

# --- Agent Configuration ---
# Specifies the DQN agent and its hyperparameters.
agent:
  name: "DQNAgent"
  params:
    # For DefeatRoachesEnv, the state is typically image-like (e.g., player_relative screen feature),
    # so a CNN is generally appropriate.
    # This 'use_cnn' flag is specific to the original DQNAgent's internal logic.
    # If you've refactored DQNAgent to use the new 'networks' module and factory,
    # you would instead specify 'network_name: "CNN"' and potentially 'network_params'.
    use_cnn: true

    batch_size: 128
    learning_rate: 0.0025 # As per the original run_dr_env.py script.
    discount_factor: 0.99
    epsilon: 1.0

    # Epsilon decay: The original script used a multiplicative decay (epsilon_decay=0.9999).
    # If you intend a linear decay over a specific number of steps,
    # this value needs to be calculated as: (initial_epsilon - min_epsilon) / total_decay_steps.
    # For now, using the multiplicative factor from the original script.
    epsilon_decay: 0.9999
    epsilon_min: 0.1

    # 'net_arch' is typically for MLP architectures. If using a CNN,
    # the architecture is defined within the CNNModel class itself.
    # net_arch: [256, 256]

    target_update_freq: 50 # How often to update the target network (in agent steps).
    memory_capacity: 10000

    # 'learn_after_steps' was a parameter in the original script.
    # If your DQNAgent implementation supports delaying learning until
    # a certain number of steps have been collected, you can add it here.
    # learn_after_steps: 5000

# --- Runner Configuration ---
# Defines how the training will be run.
runner:
  total_episodes: 500 # Number of episodes to run for training.
  is_training: true # Set to true for training mode.

  # Directory to save TensorBoard logs.
  # A timestamped sub-folder will be created automatically.
  tensorboard_log_dir: "./logs" # Or a more specific path like "./logs/dqn_defeat_roaches"

  # Directory to save trained models.
  # A timestamped sub-folder will be created automatically.
  model_save_dir: "./models"

  # How often (in episodes) to save a checkpoint of the model.
  # The original script had this disabled (set to 0).
  # Set to a positive value (e.g., 50) to enable periodic saving.
  save_model_each_episode_num: 0
